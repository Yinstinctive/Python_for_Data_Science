**Linear Regression Theory**<br>
Our goal with linear regression is to minimize the vertical distance between all data points in our line. There are lots of different ways to minimize this, (sum of squared errors, sum of absolute errors, etc.), but all these methods have general goal of minimizing this distance.<br>
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/137.png)<br>
For example, one of the most popular methods is the least squares method, which is fitted by minimizing the sum of squares of the residuals.<br>
The residuals for an observation is the difference between observation (the y-value) and the fitted line.<br>
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/138.png)<br>
**Linear Regression with Python**<br>
```Python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

USAhousing = pd.read_csv('USA_Housing.csv')
USAhousing.info()
USAhousing.describe()
USAhousing.columns

X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population']]
y = USAhousing['Price']

#Train Test Split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state=101)

#Creating and Training the Model
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train, y_train)

#Model Evaluation
print(lm.intercept_)
coeff_df = pd.DataFrame(lm.coef_, X.columns, columns = ['Coefficient'])
```
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/139.png)<br>
```Python
#Predictions from our Model
predictions = lm.predict(X_test)
plt.scatter(y_test,predictions)
```
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/140.png)<br>
```Python
#Residual Histogram
sns.displot(y_test-predictions)
```
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/141.png)<br>
**Regression Evaluation Metrics**<br>
![results](https://github.com/Yinstinctive/Python_for_Data_Science/blob/master/images/142.png)<br>
Comparing these metrics:<br>
- MAE is the easiest to understand, because it's the average error
- MSE is more popular than MAE, because MSE punishes larger errors, which tends to be useful in the real world
- RMSE is interpretable in the "y" units
```Python
from sklearn import metrics
print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))
```
